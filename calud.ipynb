{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, n_inputs: int, n_hidden: int, n_outputs: int):\n",
    "        self.ni = n_inputs\n",
    "        self.nh = n_hidden\n",
    "        self.no = n_outputs\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        self.w1 = np.random.uniform(-0.5, 0.5, (self.nh, self.ni))\n",
    "        self.w2 = np.random.uniform(-0.5, 0.5, (self.no, self.nh))\n",
    "        \n",
    "        # Weight updates initialization\n",
    "        self.dw1 = np.zeros((self.nh, self.ni))\n",
    "        self.dw2 = np.zeros((self.no, self.nh))\n",
    "        \n",
    "        # Activation storage\n",
    "        self.z1 = np.zeros(self.nh)\n",
    "        self.z2 = np.zeros(self.no)\n",
    "        self.h = np.zeros(self.nh)\n",
    "        self.o = np.zeros(self.no)\n",
    "\n",
    "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        # Forward pass through first layer\n",
    "        self.z1 = np.dot(self.w1, inputs)\n",
    "        self.h = self.sigmoid(self.z1)\n",
    "        \n",
    "        # Forward pass through second layer\n",
    "        self.z2 = np.dot(self.w2, self.h)\n",
    "        self.o = self.sigmoid(self.z2)\n",
    "        \n",
    "        return self.o\n",
    "\n",
    "    def backward(self, inputs: np.ndarray, targets: np.ndarray) -> float:\n",
    "        # Output layer deltas\n",
    "        output_deltas = (targets - self.o) * self.sigmoid_derivative(self.o)\n",
    "        \n",
    "        # Hidden layer deltas\n",
    "        hidden_deltas = np.dot(self.w2.T, output_deltas) * self.sigmoid_derivative(self.h)\n",
    "        \n",
    "        # Update weights\n",
    "        self.dw2 += np.outer(output_deltas, self.h)\n",
    "        self.dw1 += np.outer(hidden_deltas, inputs)\n",
    "        \n",
    "        # Calculate error\n",
    "        error = 0.5 * np.sum((targets - self.o) ** 2)\n",
    "        return error\n",
    "\n",
    "    def update_weights(self, learning_rate: float):\n",
    "        self.w1 += learning_rate * self.dw1\n",
    "        self.w2 += learning_rate * self.dw2\n",
    "        \n",
    "        # Reset weight updates\n",
    "        self.dw1.fill(0)\n",
    "        self.dw2.fill(0)\n",
    "\n",
    "def train_network(mlp: MLP, \n",
    "                 training_data: List[Tuple[np.ndarray, np.ndarray]], \n",
    "                 epochs: int, \n",
    "                 learning_rate: float, \n",
    "                 batch_size: int = 1) -> List[float]:\n",
    "    errors = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_error = 0\n",
    "        np.random.shuffle(training_data)\n",
    "        \n",
    "        for i in range(0, len(training_data), batch_size):\n",
    "            batch = training_data[i:i + batch_size]\n",
    "            batch_error = 0\n",
    "            \n",
    "            for inputs, targets in batch:\n",
    "                # Forward pass\n",
    "                mlp.forward(inputs)\n",
    "                # Backward pass\n",
    "                batch_error += mlp.backward(inputs, targets)\n",
    "            \n",
    "            # Update weights after batch\n",
    "            mlp.update_weights(learning_rate)\n",
    "            total_error += batch_error\n",
    "            \n",
    "        errors.append(total_error)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Error = {total_error:.6f}\")\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Error = 0.522158\n",
      "Epoch 100: Error = 0.503114\n",
      "Epoch 200: Error = 0.502905\n",
      "Epoch 300: Error = 0.502869\n",
      "Epoch 400: Error = 0.502867\n",
      "Epoch 500: Error = 0.502848\n",
      "Epoch 600: Error = 0.502828\n",
      "Epoch 700: Error = 0.502808\n",
      "Epoch 800: Error = 0.502787\n",
      "Epoch 900: Error = 0.502749\n",
      "Epoch 1000: Error = 0.502743\n",
      "Epoch 1100: Error = 0.502717\n",
      "Epoch 1200: Error = 0.502691\n",
      "Epoch 1300: Error = 0.502663\n",
      "Epoch 1400: Error = 0.502616\n",
      "Epoch 1500: Error = 0.502597\n",
      "Epoch 1600: Error = 0.502545\n",
      "Epoch 1700: Error = 0.502504\n",
      "Epoch 1800: Error = 0.502457\n",
      "Epoch 1900: Error = 0.502419\n",
      "Epoch 2000: Error = 0.502359\n",
      "Epoch 2100: Error = 0.502290\n",
      "Epoch 2200: Error = 0.502194\n",
      "Epoch 2300: Error = 0.502118\n",
      "Epoch 2400: Error = 0.501993\n",
      "Epoch 2500: Error = 0.501877\n",
      "Epoch 2600: Error = 0.501720\n",
      "Epoch 2700: Error = 0.501528\n",
      "Epoch 2800: Error = 0.501291\n",
      "Epoch 2900: Error = 0.500979\n",
      "Epoch 3000: Error = 0.500608\n",
      "Epoch 3100: Error = 0.500139\n",
      "Epoch 3200: Error = 0.499509\n",
      "Epoch 3300: Error = 0.498679\n",
      "Epoch 3400: Error = 0.497566\n",
      "Epoch 3500: Error = 0.496062\n",
      "Epoch 3600: Error = 0.494017\n",
      "Epoch 3700: Error = 0.491229\n",
      "Epoch 3800: Error = 0.487488\n",
      "Epoch 3900: Error = 0.482562\n",
      "Epoch 4000: Error = 0.476261\n",
      "Epoch 4100: Error = 0.468545\n",
      "Epoch 4200: Error = 0.459553\n",
      "Epoch 4300: Error = 0.449580\n",
      "Epoch 4400: Error = 0.439066\n",
      "Epoch 4500: Error = 0.428370\n",
      "Epoch 4600: Error = 0.417806\n",
      "Epoch 4700: Error = 0.407599\n",
      "Epoch 4800: Error = 0.397820\n",
      "Epoch 4900: Error = 0.388503\n",
      "\n",
      "Testing XOR predictions:\n",
      "Input: [0 0], Target: 0, Output: 0.2783\n",
      "Input: [1 1], Target: 0, Output: 0.5948\n",
      "Input: [1 0], Target: 1, Output: 0.5911\n",
      "Input: [0 1], Target: 1, Output: 0.6020\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# XOR training data\n",
    "xor_data = [\n",
    "    (np.array([0, 0]), np.array([0])),\n",
    "    (np.array([0, 1]), np.array([1])),\n",
    "    (np.array([1, 0]), np.array([1])),\n",
    "    (np.array([1, 1]), np.array([0]))\n",
    "]\n",
    "\n",
    "# Create and train network\n",
    "mlp = MLP(2, 4, 1)  # 2 inputs, 4 hidden units, 1 output\n",
    "errors = train_network(mlp, xor_data, epochs=5000, learning_rate=0.1)\n",
    "\n",
    "# Test network\n",
    "print(\"\\nTesting XOR predictions:\")\n",
    "for inputs, targets in xor_data:\n",
    "    output = mlp.forward(inputs)\n",
    "    print(f\"Input: {inputs}, Target: {targets[0]}, Output: {output[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Error = 108.816682\n",
      "Epoch 100: Error = 49.561317\n",
      "Epoch 200: Error = 49.502611\n",
      "Epoch 300: Error = 49.481952\n",
      "Epoch 400: Error = 49.463058\n",
      "Epoch 500: Error = 49.444629\n",
      "Epoch 600: Error = 49.427016\n",
      "Epoch 700: Error = 49.410674\n",
      "Epoch 800: Error = 49.395892\n",
      "Epoch 900: Error = 49.382854\n",
      "\n",
      "Final training error: 49.371692\n",
      "Test error: 13.023525\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate dataset\n",
    "num_samples = 500\n",
    "X = np.random.uniform(-1, 1, (num_samples, 4))\n",
    "y = np.sin(X[:, 0] - X[:, 1] + X[:, 2] - X[:, 3]).reshape(-1, 1)\n",
    "\n",
    "# Split into training and test sets\n",
    "train_size = 400\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Create training data tuples\n",
    "train_data = [(X_train[i], y_train[i]) for i in range(train_size)]\n",
    "test_data = [(X_test[i], y_test[i]) for i in range(len(X_test))]\n",
    "\n",
    "# Create and train network\n",
    "mlp = MLP(4, 5, 1)  # 4 inputs, 5 hidden units, 1 output\n",
    "train_errors = train_network(mlp, train_data, epochs=1000, learning_rate=0.05)\n",
    "\n",
    "# Calculate test error\n",
    "test_error = 0\n",
    "for inputs, targets in test_data:\n",
    "    output = mlp.forward(inputs)\n",
    "    test_error += 0.5 * np.sum((targets - output) ** 2)\n",
    "\n",
    "print(f\"\\nFinal training error: {train_errors[-1]:.6f}\")\n",
    "print(f\"Test error: {test_error:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (10,17) and (16,) not aligned: 17 (dim 1) != 16 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Create and train network\u001b[39;00m\n\u001b[1;32m     43\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLP(\u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m26\u001b[39m)  \u001b[38;5;66;03m# 17 inputs, 10 hidden units, 26 outputs\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m train_errors \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Calculate accuracy on test set\u001b[39;00m\n\u001b[1;32m     47\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(mlp, training_data, epochs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m batch_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     batch_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mbackward(inputs, targets)\n",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Forward pass through first layer\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz1 \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz1)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Forward pass through second layer\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,17) and (16,) not aligned: 17 (dim 1) != 16 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Load and prepare data\n",
    "def prepare_letter_data(file_path):\n",
    "    # Load data\n",
    "    data = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # Separate features and labels\n",
    "    X = data.iloc[:, 1:].values  # All columns except first\n",
    "    y = data.iloc[:, 0].values   # First column contains letters\n",
    "    \n",
    "    # Normalize features to [-1, 1]\n",
    "    X = (X - X.min()) / (X.max() - X.min()) * 2 - 1\n",
    "    \n",
    "    # Convert letters to one-hot encoding\n",
    "    lb = LabelBinarizer()\n",
    "    y = lb.fit_transform(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Split data into train and test sets\n",
    "def train_test_split(X, y, train_ratio=0.8):\n",
    "    n_samples = len(X)\n",
    "    n_train = int(n_samples * train_ratio)\n",
    "    \n",
    "    # Shuffle indices\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    train_idx, test_idx = indices[:n_train], indices[n_train:]\n",
    "    \n",
    "    return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "# Prepare data\n",
    "X, y = prepare_letter_data('letter-recognition.data')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Create training data tuples\n",
    "train_data = [(X_train[i], y_train[i]) for i in range(len(X_train))]\n",
    "test_data = [(X_test[i], y_test[i]) for i in range(len(X_test))]\n",
    "\n",
    "# Create and train network\n",
    "mlp = MLP(17, 10, 26)  # 17 inputs, 10 hidden units, 26 outputs\n",
    "train_errors = train_network(mlp, train_data, epochs=1000, learning_rate=0.01, batch_size=32)\n",
    "\n",
    "# Calculate accuracy on test set\n",
    "correct = 0\n",
    "total = len(test_data)\n",
    "for inputs, targets in test_data:\n",
    "    output = mlp.forward(inputs)\n",
    "    predicted_class = np.argmax(output)\n",
    "    true_class = np.argmax(targets)\n",
    "    if predicted_class == true_class:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\nTest accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
